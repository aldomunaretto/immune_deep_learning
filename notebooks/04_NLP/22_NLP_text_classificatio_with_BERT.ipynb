{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aldomunaretto/immune_deep_learning/blob/main/notebooks/04_NLP/22_NLP_text_classificatio_with_BERT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCulyartYgMt"
   },
   "source": [
    "<h1><font color=\"#113D68\" size=6>Procesamiento del Lenguaje Natural</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Clasificación de texto con BERT</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52fXJWcsYgMu"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Procesamiento de datos](#section1)\n",
    "* [2. Procesamiento en Hugging Face](#section2)\n",
    "* [3. Finetuning para clasificación de texto](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4nmVH8hYgMv"
   },
   "source": [
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6>0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxul0OTr8BrZ"
   },
   "source": [
    "Para este ejemplo, utilizaremos el mismo corpus compuesto por tramas de películas. Sin embargo, para esta tarea solo estamos empleando un subconjunto de tramas de películas: solo aquellas que corresponden a películas de comedia, drama u western.\n",
    "\n",
    "Por lo tanto, nuestro objetivo en este ejemplo es clasificar las tramas de las películas en estos tres géneros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlqLflTFYgMv"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76601,
     "status": "ok",
     "timestamp": 1717851152217,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "qBTlLq_MFk0x",
    "outputId": "5e84a463-f4c8-4dfb-f18d-1b18dd8eba93",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
      "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ys6nhmvYgMw"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lORagD-NYgMw"
   },
   "source": [
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Procesamiento de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsA1vWxjFFMF"
   },
   "source": [
    "Este archivo tiene el formato:\n",
    "```text\n",
    "Plot | \"tab\" | Label\n",
    "```\n",
    "Entonces, nuestro primer paso sería separar etiquetas y gráficos. Para esta tarea, utilizaremos la biblioteca pandas, ya que permite un procesamiento sencillo de csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1717851182442,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "8PaEav_4HXGp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_plots_tc.csv',sep=';',encoding='utf-8',encoding_errors='ignore')\n",
    "plots=df['Plot']\n",
    "labels=df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1717851184239,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "2VMYyVLnZtQr",
    "outputId": "793c1703-75b4-4f8d-cc04-cf3598685b2c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The film opens in a town on the Mexican border. A poker game is going on in the local saloon. One of the players cheats and is shot dead by another of the players, a Mexican named Pedro. In the uproar that follows Pedro is wounded as he escapes from the saloon. The sheriff is called, who tracks Pedro to his home but Pedro kills the sherriff too. While Pedro hides, his wife Juanita, is arrested on suspicion of murdering the sheriff. Pedro rescues her from the town jail and the two head for the Mexican border. Caught by the posse before they reach the border, Juanita is killed and the film ends with Pedro being arrested and taken back to town.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1717851185972,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "9Z4cCUMp3_rA",
    "outputId": "45f17aa0-1822-485d-8638-2b4ace86ff0d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'western'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB03yWqLjXQ-"
   },
   "source": [
    "De todas las muestras existentes, seleccionamos una submuestra de 500 elementos para efectos de cómputo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1717850641403,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "A3hrxX5VQACK"
   },
   "outputs": [],
   "source": [
    "plots=plots[:500]\n",
    "labels=labels[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eMApRRVjvhU"
   },
   "source": [
    "Las redes neuronales no pueden predecir sobre etiquetas de tipo cadena. Como este es un problema de clasificación de múltiples clases, necesitamos codificar nuestras etiquetas en un formato numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1717851190202,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "fVZRkcD8EEgX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "str2id={'western':0,'drama':1,'comedy':2}\n",
    "id2str={0:'western',1:'drama',2:'comedy'}\n",
    "\n",
    "list_plots=plots.fillna(\"CVxTz\").values\n",
    "indexed_labels=np.array([str2id[l] for l in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qym4WtiAj4wA"
   },
   "source": [
    "Como de costumbre, necesitamos dividir nuestros datos entre dos conjuntos: entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2062,
     "status": "ok",
     "timestamp": 1717851195599,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "WcoG1x6eHm6r"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(list_plots, indexed_labels, test_size=0.25, random_state=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMQUjyVxFpsl"
   },
   "source": [
    "Instalación de la biblioteca de transformadores para usar HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIi0fLGyYgMx"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wb5vggHYgMx"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. Procesamiento en Hugging Face</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PygvCvPspFnT"
   },
   "source": [
    "Cargamos y procesamos datos de Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11134,
     "status": "ok",
     "timestamp": 1717851209715,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "Uwp6reC4Ffg4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available #, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAqxrV7PHyxz"
   },
   "source": [
    "Para esta sesión, usaremos el modelo `bert-base-uncased`, disponible en Hugging Face. Se establece una longitud máxima de 256 tokens por muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U90ROvjVYgMx"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre el [modelo](https://huggingface.co/bert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1099,
     "status": "ok",
     "timestamp": 1717851210802,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "9ksIkbBBIFgi",
    "outputId": "5ec5815e-5263-4312-e61d-5667133b0cd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "max_lenght = 256\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YciCIyYBIetj"
   },
   "source": [
    "Ahora tokenizamos nuestras muestras de texto de entrenamiento y validación\n",
    "\n",
    "__¡Cuidado!__ Las muestras de entrada deben ser `str` o `List[str]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23804,
     "status": "ok",
     "timestamp": 1717851244631,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "XZj-Fa9yIdw7"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_features.tolist(), truncation=True, padding=True, max_length=max_lenght)\n",
    "val_encodings = tokenizer(val_features.tolist(), truncation=True, padding=True, max_length=max_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m0QSBcrMDwM"
   },
   "source": [
    "Ahora podemos construirlo en Torch. Conjunto de datos utilizando las codificaciones calculadas de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1717851247446,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "jaVqJPHHMAr0"
   },
   "outputs": [],
   "source": [
    "class OurTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1717851250699,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "Qmksz5CdNE6u"
   },
   "outputs": [],
   "source": [
    "train_dataset= OurTorchDataset(train_encodings, train_labels)\n",
    "val_dataset= OurTorchDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPvng2s1tkIw"
   },
   "source": [
    "De forma predeterminada, HuggingFace no calcula automáticamente las métricas que estamos buscando. Necesitamos definir una función personalizada para calcular el Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1717851252300,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "PatC7HHDtjnJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qef2Qx7YgMy"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZf7VqlAYgMy"
   },
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Finetuning para clasificación de texto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXZYtMnjJu0m"
   },
   "source": [
    "Ahora podemos cargar un modelo básico preentrenado de HuggingFace. ¡Asegúrese de especificar el número correcto de etiquetas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2643,
     "status": "ok",
     "timestamp": 1717851257542,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "3SHqqlx6JuWK",
    "outputId": "6d2d9f53-f6fd-4cd6-9f17-ca8c70603015"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzgXWj0BYgMy"
   },
   "source": [
    "Construimos nuestro modelo que realizará la clasificación de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1717851260363,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "H9_G7gnIUArQ"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "   output_dir='.',     # directorio de salida\n",
    "    num_train_epochs=3,              # numero total de epochs de entrenamiento\n",
    "    warmup_steps=100,                # número de pasos de preparación para el programador de tasas de aprendizaje\n",
    "    weight_decay=0.01,\n",
    "    seed=1895,             # fuerza del decaimiento de los pesos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "nWFlFzHuLwns",
    "outputId": "db133fae-2ba3-4326-e34b-f47cccc766e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1307' max='3153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1307/3153 08:19 < 11:46, 2.61 it/s, Epoch 1.24/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.549000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # el modelo de Transformers instanciado que se va a entrenar\n",
    "    args=training_args,                  # argumentos de entrenamiento, definidos anteriormente\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics      # el callback que calcula las métricas de interés\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1717207119065,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "CJZRAdQVVvdc",
    "outputId": "1d6619f3-77f7-4989-f711-d713d9f0ea02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.00038344229687936604,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_runtime': 1.804,\n",
       " 'eval_samples_per_second': 69.289,\n",
       " 'eval_steps_per_second': 8.869,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIUUQvOMWA_-"
   },
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_lenght, return_tensors='pt').to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs=model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs=outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    return id2str[probs.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1717207372657,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "Ej_1h_gTWB_Y",
    "outputId": "056b0c78-80dc-49ac-8c9d-cadcb7629897"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'western'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"The duo decide to search for the gold together, but they are apprehended by Union forces shortly after leaving the mission - Tuco yells out Confederate-supportive statements at a group of Union soldiers, as they are covered in dust, obscuring the blue color of their uniforms. The two are brought to a prison camp which Angel infiltrated as a Union sergeant in his search for Bill Carson, getting his attention when Tuco poses as Bill Carson. Tuco reveals the name of the cemetery under torture and is sent away to be killed. Knowing that Blondie would not reveal the location, Angel Eyes recruits him into his search. Tuco escapes his fate by killing Angel Eyes' henchman, and soon finds himself in an evacuated town, where Blondie, Angel Eyes, and his gang have also arrived. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G394ekLHYgMz"
   },
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
