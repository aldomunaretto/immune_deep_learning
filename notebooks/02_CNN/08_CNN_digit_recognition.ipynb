{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aldomunaretto/immune_deep_learning/blob/main/notebooks/02_CNN/08_CNN_digit_recognition.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_FLli8oV74Y"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"index\"></a>\n",
    "## Index\n",
    "\n",
    "* [Context](#section0)\n",
    "* [MNIST dataset](#section1)\n",
    "* [Load MNIST](#section2)\n",
    "* [Baseline Model with MLP](#section3)\n",
    "* [Simple CNN for the MNIST Dataset](#section4)\n",
    "* [Deeper CNN for the MNIST Dataset](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6tfPFkBV74Z"
   },
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxsJwjDDV74Z"
   },
   "source": [
    "In this project, you will learn how to develop a Deep Learning model for the task of handwritten digit recognition using the MNIST dataset. After completing this lesson, you will know:\n",
    "* How to load MNIST and develop a neural network model.\n",
    "* How to implement and evaluate a baseline CNN for MNIST.\n",
    "* How to implement an advanced Deep Learning model for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6088,
     "status": "ok",
     "timestamp": 1716568769219,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "gy7T1BToV74a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, MaxPooling2D, Conv2D\n",
    "from keras import utils, Input\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Remove warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83bq35C8V74b"
   },
   "source": [
    "---\n",
    "<a id=\"section1\"></a>\n",
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujJJf3QUV74b"
   },
   "source": [
    "MNIST consists of images of digits from a variety of scanned documents, normalized in size and centered.\n",
    "\n",
    "Each image is given in black and white with $28 × 28$ pixels (784 pixels in total). 60,000 images are used to train a model and 10,000 images to validate it.\n",
    "\n",
    "It is a digit recognition task. As such, there are 10 digits (0 to 9) or 10 classes to predict.\n",
    "\n",
    "On Rodrigo Benenson's website, there is a list of the most advanced results and links to relevant articles on MNIST and other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTn91ahlV74c"
   },
   "source": [
    "More information about the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qda80eLaV74c"
   },
   "source": [
    "Information on MNIST results from [Rodrigo Benenson](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaZo2f75V74c"
   },
   "source": [
    "---\n",
    "<a id=\"section2\"></a>\n",
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwQpOsK8V74c"
   },
   "source": [
    "The dataset is automatically downloaded the first time this function is called and stored in your home directory at `~/.keras/datasets/mnist.pkl.gz` as a 15-megabyte file.\n",
    "\n",
    "First, we will write a small script to download and visualize the first 4 images using the `mnist.load_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1716568987055,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "KZoPXvR_V74d",
    "outputId": "a75147bf-0c9b-4f23-98ea-07e767064852"
   },
   "outputs": [],
   "source": [
    "# split the data into training (60,000) and testing (10,000) data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn_QRrXnV74d"
   },
   "source": [
    "---\n",
    "<a id=\"section3\"></a>\n",
    "## Baseline Model with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYntdBA1V74e"
   },
   "source": [
    "We will use a classic MLP as a baseline for comparison with convolutional neural network models.\n",
    "\n",
    "We import the classes, functions, and the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4WGZkH2V74e"
   },
   "source": [
    "For a classic MLP, we need to reduce the images to a vector of pixels. In this case, the $28 × 28$ images will be input vectors of 784 pixels.\n",
    "\n",
    "We perform this transformation using the `reshape()` function.\n",
    "\n",
    "The pixel values are integers, so we convert them to floating-point to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1716569311316,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "aWlMafc4V74e"
   },
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1716569324311,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "XxoBrdVigQ8f",
    "outputId": "c6cd8cf9-e3c8-4d37-e973-8ad92f4297cb"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7PR4rbXV74e"
   },
   "source": [
    "The pixel values are in a grayscale range between 0 and 255. We can normalize the pixel values in the range 0 to 1 by dividing each value by the maximum value, i.e., 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1716569344720,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "CbPNnLUXV74e"
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1716569352203,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "PgeCUiFpgZv2",
    "outputId": "b57af880-9f3e-4d6d-b746-4f00ad9831f9"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY7FT9KSV74e"
   },
   "source": [
    "Finally, the output variable is an integer from 0 to 9. Therefore, we will use One-Hot Encoding to transform the vector of class integers into a binary matrix.\n",
    "\n",
    "We will use the Keras function `np_utils.to_categorical()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1716569503337,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "oODAVcV3V74e"
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "num_classes = 10\n",
    "y_train = utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsvEbpLeV74e"
   },
   "source": [
    "Let's define our model:\n",
    "1. The number of inputs will be the maximum pixel size (784).\n",
    "2. It will have a hidden layer with the same number of neurons as inputs (784).\n",
    "3. A ReLU activation function will be used in the hidden layer.\n",
    "4. A Softmax activation function will be used in the output layer.\n",
    "5. The loss function will be `categorical_crossentropy`.\n",
    "6. We will use ADAM to learn the weights.\n",
    "\n",
    "![Baseline MLP](https://raw.githubusercontent.com/aldomunaretto/immune_deep_learning/main/image/notebooks/baselineMLP.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1716569681141,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "DhmZU8LVV74e"
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential(\n",
    "         [\n",
    "              Input(shape=(num_pixels,)),\n",
    "              Dense(num_pixels, activation='relu'),\n",
    "              Dense(num_classes, activation='softmax')\n",
    "         ]\n",
    "    )\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dotBjgC8V74e"
   },
   "source": [
    "We train and evaluate the model.\n",
    "1. The model is fitted over 10 epochs with updates every 200 images.\n",
    "2. The test data is used as the validation dataset.\n",
    "3. A `verbose` value of 2 is used.\n",
    "4. We evaluate on the test set and print the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85289,
     "status": "ok",
     "timestamp": 1716569771117,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "Vl1mzUO7V74e",
    "outputId": "0de44ba3-bca0-4c85-afe5-358af61e324a"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Error of the Baseline Model: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEVOrsxhV74f"
   },
   "source": [
    "---\n",
    "<a id=\"section4\"></a>\n",
    "## Simple CNN for the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONAWtT68V74f"
   },
   "source": [
    "Now that we have seen how to load the MNIST dataset and train a simple multilayer perceptron model on it, it is time to develop a more sophisticated convolutional neural network or CNN model.\n",
    "\n",
    "We will create a simple CNN for MNIST that demonstrates how to use all aspects of a modern CNN implementation, including convolutional layers, pooling layers, and dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r49KUaX-V74f"
   },
   "source": [
    "In Keras, the layers used for two-dimensional convolutions expect pixel values with dimensions `[samples]-[width]-[height]-[channels]`\n",
    "\n",
    "As for the channel in MNIST, since it is given in grayscale, the pixel dimension is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1716570383915,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "PxRU1AvxV74f"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbcZfBzfV74f"
   },
   "source": [
    "We normalize the pixel values in the range 0 to 1 and perform OHE on the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1716570412280,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "dnF0EqbEV74f"
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# one hot encode outputs\n",
    "numero_clases = 10\n",
    "y_train = utils.to_categorical(y_train, num_classes=numero_clases)\n",
    "y_test = utils.to_categorical(y_test, num_classes=numero_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AFIhOmkV74f"
   },
   "source": [
    "Next, we define our neural network model:\n",
    "1. The first hidden layer is a convolutional layer called `Conv2D`.\n",
    "    * It has 32 feature maps, with a size of $5 × 5$ and a ReLU activation function.\n",
    "2. Pooling layer `MaxPooling2D`.\n",
    "    * Patch size of $2 × 2$.\n",
    "3. Regularization layer `Dropout`.\n",
    "4. Flatten layer to convert the 2D matrix into a vector (1D).\n",
    "5. Dense layer with 128 neurons and ReLU activation function.\n",
    "6. Output layer with 10 neurons for the 10 classes and a **Softmax** activation function.\n",
    "7. Compilation with ADAM, logarithmic loss as the cost function, and Accuracy as the metric.\n",
    "\n",
    "![Simple CNN Topology](https://raw.githubusercontent.com/aldomunaretto/immune_deep_learning/main/image/notebooks/cnnTopology.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1716571200911,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "EFgiHbXHV74f"
   },
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "def simple_model():\n",
    "    # create model\n",
    "   model = Sequential(\n",
    "         [\n",
    "              Input(shape=(28,28,1)),\n",
    "              Conv2D(32, (5,5), activation='relu'),\n",
    "              MaxPooling2D(),\n",
    "              Dropout(0.2),\n",
    "              Flatten(),\n",
    "              Dense(128, activation='relu'),\n",
    "              Dense(num_classes, activation='softmax')\n",
    "         ]\n",
    "   )\n",
    "    # Compile model\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JddY2cZV74g"
   },
   "source": [
    "We train with 10 epochs at a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326359,
     "status": "ok",
     "timestamp": 1716571537706,
     "user": {
      "displayName": "Aldo Munaretto",
      "userId": "00686307437549586015"
     },
     "user_tz": -120
    },
    "id": "P9AgGIdTV74j",
    "outputId": "470b4a5f-fda2-4f8e-a77f-c84e81dcce31"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = simple_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Error of the CNN Simple Model: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQCkPowGV74j"
   },
   "source": [
    "---\n",
    "<a id=\"section5\"></a>\n",
    "## Deeper CNN for the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJG6lD4RV74j"
   },
   "source": [
    "This time we define an architecture with more convolutional layers, Max-pooling, and fully connected layers.\n",
    "1. Convolutional layer with 30 feature maps of size $5 × 5$.\n",
    "2. Pooling layer with a patch size of $2 × 2$.\n",
    "3. Convolutional layer with 15 feature maps of size $3 × 3$.\n",
    "4. Pooling layer with a patch size of $2 × 2$.\n",
    "5. Dropout layer with 20%.\n",
    "6. Flatten layer.\n",
    "7. Fully connected layer with 128 neurons and ReLu activation.\n",
    "8. Fully connected layer with 50 neurons and ReLu activation.\n",
    "9. Output layer with Softmax activation.\n",
    "10. Compilation with ADAM, logarithmic loss as the cost function, and Accuracy as the metric.\n",
    "\n",
    "![Deeper CNN Topology](https://raw.githubusercontent.com/aldomunaretto/immune_deep_learning/main/image/notebooks/cnnlarger.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKEAFm-bV74k"
   },
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    # create model\n",
    "    ???\n",
    "    # Compile model\n",
    "    ???\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "???\n",
    "\n",
    "# Fit the model\n",
    "???\n",
    "\n",
    "# Final evaluation of the model\n",
    "???"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
