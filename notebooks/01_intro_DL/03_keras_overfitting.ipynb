{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_7YU02C3OYF"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aldomunaretto/immune_deep_learning/blob/main/notebooks/01_intro_DL/03_keras_overfitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PZJGTFJ3OYL"
   },
   "source": [
    "# Artificial Neural Networks - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevent Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
    "from keras.utils import set_random_seed\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRHNqdZi3OYO"
   },
   "source": [
    "We will use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), consisting of a collection of 28x28 pixel images corresponding in digits from 0 to 9 manuscripts. The purpose of this data set is to train models that recognize handwritten numbers.\n",
    "\n",
    "We will train, therefore, an [OCR (Optical Character Recognition)](https://en.wikipedia.org/wiki/Optical_character_recognition) model for multiclass classification (numbers from 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2ygDPx13OYP",
    "outputId": "9a756b5d-b47a-40a4-f1c8-8dc5888324f2"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBbn20_a3OYP",
    "outputId": "dbcabcd9-d68e-4abb-c515-b51bd689a3c0"
   },
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41AdadXs3OYQ"
   },
   "source": [
    "As they are 8-bit images, the colors (in this case only one channel, the gray channel) of each pixel are encoded with a value between 0 and 255, with 0 being black and 255 being white. It is usual to normalize the values to work with a range between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E7HfFvi3OYQ"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc8_WxL03OYR"
   },
   "source": [
    "We visualize a random image of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "YJR0dmk53OYS",
    "outputId": "9f10b88d-6fa5-4ca9-9287-bf867e7daba6"
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(x_train))\n",
    "plt.imshow(x_train[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iZldKdZ3OYS"
   },
   "source": [
    "We reduce the dataset for  inducing more overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Y1nrs043OYT",
    "outputId": "429144d9-3bde-44be-b248-20dc9c56ca27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "_, x, _, y = train_test_split(\n",
    "    x_train, y_train, test_size=0.02, random_state=1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples for each class\n",
    "num_classes = 10\n",
    "x.shape, y.shape, Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWCkISrx3OYT"
   },
   "source": [
    "#### Why do we need to use regularization?\n",
    "\n",
    "We are going to assign the classes randomly to each image with random shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jC3XcIG3OYU",
    "outputId": "fdb3233f-f6f9-4c3f-e78b-faa3bdf5bc63"
   },
   "outputs": [],
   "source": [
    "y_shuffle = np.copy(y)\n",
    "np.random.shuffle(y_shuffle)\n",
    "y[:5], y_shuffle[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "zRNwWyIZ3OYU",
    "outputId": "2f125b36-f747-4131-953a-d8f39c406a9e"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 9):\n",
    "    ind = np.random.randint(len(y_shuffle))\n",
    "    axes[i].imshow(x[ind].reshape(28, 28))\n",
    "    axes[i].set_title(\n",
    "        f\"Digit = {y_shuffle[ind]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-rIuA4w3OYU"
   },
   "source": [
    "Lets build a model for learning the new random classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN6MbjnH3OYV"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28, 28))\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(64, activation='relu')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(64, activation='relu')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(64, activation='relu')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(10, activation='softmax')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LtvPX5p3OYV",
    "outputId": "c9a52ec2-053d-430e-ada8-220408d46042"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "history = model.fit(\n",
    "    x,\n",
    "    y_shuffle,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGiG_R1B3OYW",
    "outputId": "c99bdd4f-459e-4062-af07-1222daf32e26"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x, y_shuffle, verbose=0)\n",
    "print('Train Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "S0zsJrtF3OYW",
    "outputId": "ae8dad90-6f57-4d08-e021-dd47928accf9"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 9):\n",
    "    ind = np.random.randint(len(y_shuffle))\n",
    "    pred_pobs = model.predict(np.expand_dims(x[ind], 0)).flatten()\n",
    "    pred_class = np.argmax(pred_pobs)\n",
    "    prob = np.max(pred_pobs)\n",
    "    axes[i].imshow(x[ind].reshape(28, 28))\n",
    "    axes[i].set_title(\n",
    "        f\"Digit = {y_shuffle[ind]} \\n Prediction={pred_class} , Prob={prob:.4f}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5EC6O8K3OYX"
   },
   "source": [
    "We can see that the model has fully fitted the training set. It practically does not matter what data you introduce into the neural network, it will almost always fit. The objective is not to fit the training set, it is to **GENERALIZE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkt3eGa33OYX"
   },
   "source": [
    "#### Regularization for preventing overfitting\n",
    "\n",
    "To get the model to **generalize**, we can use one of the numerous regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWC4tzLH3OYX"
   },
   "source": [
    "We build a model consisting of:\n",
    "\n",
    "- Input with dimension (28,28)\n",
    "- Flatten layer\n",
    "- Dense hidden  layer with 256 neurons and ReLU as activation function\n",
    "- Second dense hidden layer with 128 neurons and ReLU as activation function\n",
    "- Third dense  hidden layer with 128 neurons and ReLU as activation function\n",
    "- Output layer with a single neuron that implements the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GKOdU1F3OYY"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTOOs8ab3OYY",
    "outputId": "5e10a499-9037-4c91-d3de-d1ac90ee2bda"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIXnZgIz3OYY"
   },
   "source": [
    "We define the following configuration for training:\n",
    "- **optimizer**: adam\n",
    "- **loss function**: sparse_categorical_crossentropy\n",
    "- **metrics**: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYaUHX6w3OYY"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zePth3HE3OYZ",
    "outputId": "f0b85b15-b9df-41d5-932d-38dddcd10af8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mxCdX7y3OYZ",
    "outputId": "c190db1d-cd00-4257-e6c9-c0ab5ecc3070"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9rUiXiZ3OYZ"
   },
   "source": [
    "We visualize the evolution of the values in each epoch of the loss function and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l4oaVzH3OYa"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uIOTYfZF3OYa",
    "outputId": "404fc05d-1787-4c37-f21f-673ca7b717ba"
   },
   "outputs": [],
   "source": [
    "hist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDvoCIOf3OYa"
   },
   "source": [
    "<a id='overfitting_results'></a>\n",
    "When interpreting both graphs, it is evident that the model over-fits the training values and is not able to generalize well for the validation values. **It is a clear example of overfitting**.\n",
    "\n",
    "The following function will be useful throughout the different practical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swBENOfM3OYa"
   },
   "outputs": [],
   "source": [
    "def show_loss_accuracy_evolution(history):\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_samples(x_test, y_test, model):\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, -1)\n",
    "\n",
    "    L = 5\n",
    "    W = 5\n",
    "    fig, axes = plt.subplots(L, W, figsize=(14, 14))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in np.arange(0, L * W):\n",
    "        ind = np.random.randint(len(y_test))\n",
    "        axes[i].imshow(x_test[ind].reshape(28, 28))\n",
    "        prob_pred = np.max(predictions[ind, :])\n",
    "        class_pred = int(predicted_classes[ind])\n",
    "        original_class = int(y_test[ind])\n",
    "        if class_pred == original_class:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        title = \"Pred: {0} \\n Target: {1} \\n Prob: {2:.3f}\".format(\n",
    "        class_pred, original_class, prob_pred)\n",
    "        axes[i].set_title(title, color=color)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "WRF3_oea3OYb",
    "outputId": "a2a96ba5-9be5-429c-b533-739277ce4939"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvFmDK623OYb",
    "outputId": "c99dc29d-82b0-48b1-8313-45c67c2c5019"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EQsxCIhU3OYb",
    "outputId": "8b79d0fa-230c-4ab1-c58d-b3e79617dcf2"
   },
   "outputs": [],
   "source": [
    "show_samples(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G_slG7N3OYb"
   },
   "source": [
    "## Create a simpler model\n",
    "\n",
    "The simplest way to prevent overfitting is to create  a simple model: A model with less number of  parameters (which is determined by the number of layers and the number of units per layer).\n",
    "\n",
    "A more complex model with more parameters will have much more capacity and will be able to learn almost any training set. Deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnKFIUQg3OYc",
    "outputId": "983cdf43-c3ef-4640-f38d-08b421dade7f"
   },
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=(28, 28), name='input_layer')\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# First hidden layer\n",
    "l_1 = Dense(8, activation='relu', name='layer_1')(flat)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_1)\n",
    "\n",
    "# Model definition\n",
    "model_simpler = Model(inputs=inputs, outputs=outputs, name='simpler_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model summary\n",
    "model_simpler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuLaV7rg3OYe",
    "outputId": "55500c98-87d3-44c3-fe50-bd4c88c0a87a"
   },
   "outputs": [],
   "source": [
    "model_simpler.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "history_simpler = model_simpler.fit(x,\n",
    "                                    y,\n",
    "                                    batch_size=64,\n",
    "                                    epochs=100,\n",
    "                                    validation_split=0.25,\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "BbBDIzoi3OYe",
    "outputId": "5194d1e8-9f22-45b2-e5ec-deaa4ea54ea3"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAquK94j3OYe",
    "outputId": "9c9cf0db-3505-47a9-d38d-6e0ecda6c92e"
   },
   "outputs": [],
   "source": [
    "results = model_simpler.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaDZdAub3OYf"
   },
   "source": [
    "## Reduce batch_size\n",
    "\n",
    "Other method is decreasing the `batch_size`during the gradient descend to add more uncertainty to the estimation of the parameter gradients, thus reducing the capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuHwbbNG3OYf",
    "outputId": "7a0d7acd-fed7-4eed-dfd8-1db1787accf7"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADoBEHbr3OYf",
    "outputId": "fc00f063-f00b-43e0-f2ee-c8f42a14f736"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_2 = model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=16,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "TcBN1Zx-3OYg",
    "outputId": "248bdf3b-c378-40c9-db52-69e34601d010"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8C6n-e-k3OYg",
    "outputId": "b5f2d28c-c48a-43ff-b4e6-ea29c6223c65"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFcC18G93OYg"
   },
   "source": [
    "### Practice 1:\n",
    "- What differences do you see if you change the batch_size to a lower or higher value? For example to 1 and to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thfr2NKc3OYg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zclThbN3OYg"
   },
   "source": [
    "<a id='dropout'></a>\n",
    "## Dropout\n",
    "\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (set to zero) a number of output features of the layer during training with a probability **p** (`dropout rate`).\n",
    "\n",
    "In `keras` you can introduce dropout in a network via the [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), which gets applied to the output of layer right before.\n",
    "\n",
    "```python\n",
    "keras.layers.Dropout(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = Dropout(0.4)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(Dropout(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX7nGDB73OYh"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "flat = Dropout(0.5, name='dropout_flat')(flat)\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "l_1 = Dropout(0.5, name='dropout_l1')(l_1)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "l_2 = Dropout(0.5, name='dropout_l2')(l_2)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "l_3 = Dropout(0.5, name='dropout_l3')(l_3)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_dropout = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNRnE_pq3OYh",
    "outputId": "8018d2e7-034b-4ef5-f0c2-6c7389314b21"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAuqM_ef3OYh"
   },
   "outputs": [],
   "source": [
    "model_dropout.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mt8BTrKO3OYh",
    "outputId": "379b37b1-27ba-42ca-fe69-d860d7fbd631"
   },
   "outputs": [],
   "source": [
    "history_dropout = model_dropout.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "9RfBiVeZ3OYi",
    "outputId": "2adb18c9-ca0f-4c4c-da4c-2666d624dea4"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XojHrVhy3OYi",
    "outputId": "2da9f7a9-da1b-48d3-cfbe-977c8449b8f1"
   },
   "outputs": [],
   "source": [
    "results = model_dropout.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWxAZ3dn3OYi"
   },
   "source": [
    "### Practice 2:\n",
    "- What happens if you change the dropout to 0.1 or 0.9? \n",
    "- And what if you put it only on one layer?\n",
    "- What happens if we delete the flatten first dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDuMP7np3OYi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1S1Jlrc3OYj"
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "[Batch normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
    "\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.BatchNormalization()(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.BatchNormalization())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ELx-VHG3OYj"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "l_1 = BatchNormalization()(l_1)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "l_2 = BatchNormalization()(l_2)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_batch_norm = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_batch_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzzg_ot23OYj",
    "outputId": "eef98b74-971a-4c71-b674-bd91a1380e6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_batch_norm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agh0beK63OYj"
   },
   "outputs": [],
   "source": [
    "model_batch_norm.compile(optimizer='adam', \n",
    "                         loss='sparse_categorical_crossentropy', \n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpkMM6943OYk",
    "outputId": "7f75d5ae-4034-4a17-99e8-3c57a669d5f5"
   },
   "outputs": [],
   "source": [
    "history_batch_norm = model_batch_norm.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "opAKrkjx3OYk",
    "outputId": "2fee37d1-e3dc-4d43-80a4-ef443f30e99b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPqtBjeq3OYm",
    "outputId": "7dfc5075-7a3d-4995-962b-255a6708bbf9"
   },
   "outputs": [],
   "source": [
    "results = model_batch_norm.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlDM-dCa3OYm"
   },
   "source": [
    "<a id='l1_l2_elasticnet'></a>\n",
    "## L1, L2 and ElasticNet\n",
    "\n",
    "Remember that during training the configuration of weights and biases is learned to improve the results for a given loss function.\n",
    "\n",
    "* [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization), where the cost added is proportional to the absolute value of the weights coefficients (\"L1 norm\" of the weights).\n",
    "\n",
    "```python\n",
    "keras.regularizers.l1(l1=0.01)\n",
    "\n",
    "Dense(3, kernel_regularizer='l1')\n",
    "\n",
    "```\n",
    "\n",
    "* [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization), where the cost added is proportional to the square of the value of the weights coefficients ( \"L2 norm\" of the weights).\n",
    "\n",
    "```python\n",
    "keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "Dense(3, kernel_regularizer='l2')\n",
    "\n",
    "```\n",
    "* L1_L2 or Elastic Net.\n",
    "\n",
    "```python\n",
    "keras.regularizers.l1_l2(\n",
    "    l1=0.01, l2=0.01\n",
    ")\n",
    "\n",
    "Dense(3, kernel_regularizer='l1_l2')\n",
    "\n",
    "```\n",
    "\n",
    "Furthermore, it is possible to choose whether to include the penalty in the cost function on the weights, the biases or on the activation, with the following arguments:\n",
    "- `kernel_regularizer`: only on weights.\n",
    "- `bias_regularizer`: only on biases.\n",
    "- `activity_regularizer`: on full output.\n",
    "\n",
    "[link to documentation](https://keras.io/api/layers/regularizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbXrPxqa3OYn"
   },
   "outputs": [],
   "source": [
    "kerner_regularizer_l1 = l1_l2(l1=1e-5, l2=5e-4)\n",
    "kerner_regularizer_l2 = l2(5e-4)\n",
    "kerner_regularizer_l3 = l1(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNsA0j643OYn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Input\n",
    "inputs = Input(shape=(28, 28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', kernel_regularizer=kerner_regularizer_l1, name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', kernel_regularizer=kerner_regularizer_l2, name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', kernel_regularizer=kerner_regularizer_l3, name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_regularizers = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_regularizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr4zca0V3OYn",
    "outputId": "f130975d-b5cd-4f83-cae8-263f9fb1885c"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_regularizers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3lvKJAd3OYn"
   },
   "outputs": [],
   "source": [
    "model_regularizers.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10xmvmdV3OYo",
    "outputId": "b678436a-bf51-4a10-ac81-63cfa2332e3a"
   },
   "outputs": [],
   "source": [
    "history_regularizers = model_regularizers.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIvaheyQ3OYo"
   },
   "source": [
    "Lets compare the weights norms of the model without regularization and with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwhxqPG03OYo",
    "outputId": "150ac681-e193-4dae-8d67-cb6db94e73d7"
   },
   "outputs": [],
   "source": [
    "for ly in model.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Mpda7Rd3OYo",
    "outputId": "6472820a-3f18-4371-8286-95274a6d0aa9"
   },
   "outputs": [],
   "source": [
    "for ly in model_regularizers.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwuiXWtN3OYo",
    "outputId": "5a0a3efa-01a1-45f2-9e3d-b7358c189fe5"
   },
   "outputs": [],
   "source": [
    "for ly in model_batch_norm.layers:\n",
    "    if 'layer_' in ly.name:\n",
    "        W, b = ly.get_weights()\n",
    "        l2_norm = np.sum(W**2)**0.5\n",
    "        zero_elems = len(W[np.abs(W) < 1e-14])\n",
    "        sparsity = zero_elems / np.prod(W.shape)\n",
    "        sparsity = np.round(100 * sparsity, 2)\n",
    "        print('L2 norm {0} weights: {1}, shape:{2}, sparsity:{3}'.format(\n",
    "            ly.name, l2_norm, W.shape,sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "_5_UxN3G3OYp",
    "outputId": "615785a3-19ed-417f-c9ec-7357412c8b82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_regularizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSDWsxL03OYp",
    "outputId": "8e0f5077-111c-4a2e-d5b2-15dab8730cf5"
   },
   "outputs": [],
   "source": [
    "results = model_regularizers.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqX1zVzc3OYq"
   },
   "source": [
    "### Practice 3:\n",
    "- What happens if you change the regularizers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTO9REa83OYq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jVecDe93OYq"
   },
   "source": [
    "#### Combine L2 Regularization and Dropout\n",
    "\n",
    "Use L2:\n",
    "```python\n",
    "kerner_regularizer_l2 = l2(5e-4)\n",
    "```\n",
    "And Dropout:\n",
    "\n",
    "```python\n",
    "next_layer = Dropout(0.4)(prev_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6UnozT63OYq"
   },
   "outputs": [],
   "source": [
    "kerner_regularizer_l2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "tppm0Evq3OYr",
    "outputId": "5188af27-4056-426a-a9e3-e38d9f8bf016"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(), name='input_layer')\n",
    "\n",
    "# Convert the 2D image to a vector\n",
    "flat = (inputs)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_1 = Dense(128, activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_1')(flat)\n",
    "\n",
    "# Add dropout\n",
    "l_1 = ...(l_1)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_2 = Dense(64,\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_2')(l_1)\n",
    "# Add dropout\n",
    "l_2 = ...(l_2)\n",
    "\n",
    "\n",
    "# Add L2-normalization\n",
    "l_3 = Dense(32,\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=...,\n",
    "                   name='layer_3')(l_2)\n",
    "\n",
    "# Add dropout\n",
    "l_3 = ...(l_3)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(..., activation=...,\n",
    "                       name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_combination = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_regularizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biU4LJZf3OYr"
   },
   "outputs": [],
   "source": [
    "model_combination.compile(optimizer='adam', \n",
    "                          loss='sparse_categorical_crossentropy', \n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjR9ew203OYr"
   },
   "outputs": [],
   "source": [
    "history_combination = model_combination.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3ZN5y3H3OYr"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Wh5F3ax3OYr"
   },
   "outputs": [],
   "source": [
    "results = model_combination.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmU0JxNh3OYs"
   },
   "source": [
    "## Noise injection\n",
    "\n",
    "Apply additive zero-mean Gaussian noise.\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = GaussianNoise(stddev)(prev_layer)\n",
    "```\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(GaussianNoise(stddev)())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdRQfUGL3OYs"
   },
   "outputs": [],
   "source": [
    "stddev = 0.2\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "l_1 = GaussianNoise(stddev, name='noise_l1')(l_1)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "l_2 = GaussianNoise(stddev, name='noise_l2')(l_2)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "l_3 = GaussianNoise(stddev, name='noise_l3')(l_3)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_noise = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkEXUHb33OYs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_noise.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BfQH4nj3OYs"
   },
   "outputs": [],
   "source": [
    "model_noise.compile(optimizer='adam', \n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd-2dp-F3OYs"
   },
   "outputs": [],
   "source": [
    "history_noise = model_noise.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVKHt0ye3OYt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ST_waVY3OYt"
   },
   "outputs": [],
   "source": [
    "results = model_noise.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjMmAvbn3OYt"
   },
   "source": [
    "<a id='early_stopping'></a>\n",
    "## Early Stopping\n",
    "\n",
    "Another of the most used techniques in neural network training is **early stopping**. It basically consists of stopping the training process in an Epoch prior to the one defined in the compilation.\n",
    "\n",
    "This, in addition to resulting in a model with less overfitting, saves unnecessary computation time.\n",
    "\n",
    "To do this, we can use the [Keras callbacks](https://keras.io/api/callbacks/), objects that perform actions at different times during training. Specifically, we will make use of [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/).\n",
    "\n",
    "```python\n",
    "keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    restore_best_weights=False,\n",
    "    verbose=0,\n",
    ")\n",
    "```\n",
    "\n",
    "- **monitor**: Quantity to be monitored. Depending on the evolution, it will be decided to stop training.\n",
    "\n",
    "- **min_delta**: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "\n",
    "- **patience**: Number of epochs with no improvement after which training will be stopped.\n",
    "\n",
    "- **restor_best_weights**: Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    "\n",
    "- **verbose**: verbosity mode.\n",
    "\n",
    "There are other callbacks that can be very useful such as:\n",
    "- [`ModelCheckpoint`](https://keras.io/api/callbacks/model_checkpoint/), which allows storing the state of a model at different times of training. This is very useful in trainings that can take hours or days.\n",
    "- [`TensorBoard`](https://keras.io/api/callbacks/tensorboard/), which allows the use of [TensorBoard](https://www.tensorflow.org/tensorboard?hl=es-419), framework TensorFlow for visualizing metrics and evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6zeKAEN3OYt"
   },
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    restore_best_weights=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2rA89iF3OYt"
   },
   "source": [
    "Lets create the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v85E5eS43OYu"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_early_stopping = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_early_stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MQYj5Dm3OYu"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_early_stopping.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6E61RKXr3OYu"
   },
   "outputs": [],
   "source": [
    "model_early_stopping.compile(optimizer='adam',\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkfU5lZv3OYu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_early_stopping = model_early_stopping.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNF5NU2X3OYu"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rhoc1-43OYv"
   },
   "outputs": [],
   "source": [
    "results = model_early_stopping.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCeFGv0bIQUW"
   },
   "source": [
    "<a id='learning_rate'></a>\n",
    "## Adjusting the learning rate\n",
    "\n",
    "By adjusting the learning rate we can obtain better results and avoid falling into local minimums.\n",
    "\n",
    "```python\n",
    "keras.optimizers.Adam(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "We can use alsa `SGD` optimizer or any other optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2SkAholIPtN"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_learning_rate = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_learning_rate.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHz2FWwnLerl"
   },
   "outputs": [],
   "source": [
    "model_learning_rate.compile(optimizer=Adam(learning_rate=0.0003),\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNf1aygMLuu_"
   },
   "outputs": [],
   "source": [
    "history_learning_rate = model_learning_rate.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AfgOlfoLxMp"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9mECyL_LxwA"
   },
   "outputs": [],
   "source": [
    "results = model_learning_rate.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngGzHwCXL-bS"
   },
   "source": [
    "<a id='weight_decay'></a>\n",
    "## Weight decay\n",
    "\n",
    "By adding a penalty to the loss function through weight decay, we allow the network to be optimized while its weights do not increase too much.\n",
    "\n",
    "```python\n",
    "keras.optimizers.Adam(weight_decay=None)\n",
    "```\n",
    "\n",
    "We can use alsa `SGD` optimizer or any other optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jq8PHHt6L8M0"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(28,28), name='input_layer')\n",
    "# Convert the 2D image to a vector\n",
    "flat = Flatten()(inputs)\n",
    "\n",
    "# hidden layer 1\n",
    "l_1 = Dense(256, activation='relu', name='layer_1')(flat)\n",
    "# hidden layer 2\n",
    "l_2 = Dense(128, activation='relu', name='layer_2')(l_1)\n",
    "# hidden layer 3\n",
    "l_3 = Dense(128, activation='relu', name='layer_3')(l_2)\n",
    "\n",
    "# Outputs\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(l_3)\n",
    "\n",
    "# Model definition\n",
    "model_weight_decay = Model(inputs=inputs, outputs=outputs, name='dont_overfit_model_weight_decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_weight_decay.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTl-0xpmMhx4"
   },
   "outputs": [],
   "source": [
    "model_weight_decay.compile(optimizer=Adam(weight_decay=1e-4),\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayu54ECOMoOA"
   },
   "outputs": [],
   "source": [
    "history_weight_decay = model_weight_decay.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow6e-IiEMqU7"
   },
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fisU3yB2Mr5R"
   },
   "outputs": [],
   "source": [
    "results = model_weight_decay.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAR4CniU3OYv"
   },
   "source": [
    "<a id='tensorboard'></a>\n",
    "# TensorBoard\n",
    "\n",
    "Load TensorBoard notebook extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiYjZktN3OYv"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjR2aASs3OYv"
   },
   "source": [
    "\n",
    "We build the model. The input layer will have a dimension of `28x28`. The output layer, in a multiclass classification problem with 10 possible classes, will be made up of 10 neurons with [softmax activation function](https://keras.io/api/layers/activations/#softmax-function ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB2q74T33OYv"
   },
   "outputs": [],
   "source": [
    "kernel_regularizer_l2 = l2(5e-4)\n",
    "\n",
    "model = Sequential(name=\"tensorboard_model\")\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hidden Layers + regularization\n",
    "model.add(Dense(64, activation='relu', name='layer_1'))\n",
    "model.add(Dense(64, activation='relu',kernel_regularizer=kernel_regularizer_l2, name='layer_2'))\n",
    "# output layer\n",
    "model.add(Dense(10, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7e6SMP83OYw"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSRRt6sb3OYw"
   },
   "source": [
    "In the [example with early stopping](#early_stopping) the Keras callbacks were presented. To use TensorBoard we will need to include the [`TensorBoard` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) to the training.\n",
    "\n",
    "This callback will create and store the logs in the directory that we indicate.\n",
    "\n",
    "By default it will create a directory called \"logs\" in the directory where this notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IR8KnWU3OYw"
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jur7vrqP3OYw"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWJgOunk3OYw"
   },
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=10,  # if during 10 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    restore_best_weights=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa1UqhwG3OYw"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUYKIxDC3OYw"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[tensorboard_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epaDYV-t3OYx"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25LQMlmu3OYx"
   },
   "source": [
    "To ** start TensorBoard ** we can execute the following cell or, from the command line, execute:\n",
    "\n",
    "    tensorboard --logdir logs / fit\n",
    "\n",
    "An application will be launched that will be listening by default on port 6006. It is possible to use TensorBoard by opening [localhost: 6006 /](http://localhost:6006/) in a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8dzUWSs3OYx"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0pzsVCj3OYx"
   },
   "source": [
    "A brief overview of the dashboards shown (tabs in top navigation bar):\n",
    "\n",
    "* The **Scalars** dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
    "* The **Graphs** dashboard helps you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly.\n",
    "* The **Distributions** and **Histograms** dashboards show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLR8Bsz93OYx"
   },
   "source": [
    "### Practice 4 \n",
    "- Improve the last model with some regularization techniques to obtain at least `0.98` of test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w9I738V3OYx"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hidden Layers + regularization\n",
    "model.add(...)\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUtJ3R433OYx"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSQh5bTX3OYy"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64\n",
    ")\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Biv6Qjyz3OYy"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=...,\n",
    "    validation_split=0.2,\n",
    "    batch_size=...,\n",
    "    callbacks=...\n",
    ")\n",
    "results = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBKOTaB33OYy"
   },
   "source": [
    "### Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3Yhbm3C3OYy"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predicted_classes = np.argmax(predictions, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3r2AV5a3OYy"
   },
   "outputs": [],
   "source": [
    "L = 5\n",
    "W = 5\n",
    "fig, axes = plt.subplots(L, W, figsize=(14, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, L * W):\n",
    "    ind = np.random.randint(len(y_test))\n",
    "    axes[i].imshow(x_test[ind].reshape(28, 28))\n",
    "    prob_pred = np.max(predictions[ind, :])\n",
    "    class_pred = int(predicted_classes[ind])\n",
    "    original_class = int(y_test[ind])\n",
    "    if class_pred == original_class:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    title = \"Pred: {0} \\n Target: {1} \\n Prob: {2:.3f}\".format(\n",
    "    class_pred, original_class, prob_pred)\n",
    "    axes[i].set_title(title, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
